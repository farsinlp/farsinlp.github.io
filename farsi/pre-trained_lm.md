# Pre-trained Language Models

The intuition behind pre-trained language models is to create a black box which understands the language and can then be asked to do any specific task in that language.

| Title | Description |
| ----- | ----------- |
| ParsBERT<br>[[website]](https://github.com/hooshvare/parsbert) [[paper]](https://arxiv.org/abs/2005.12515) | ParsBERT is a monolingual language model based on Googleâ€™s BERT architecture. This model is pre-trained on large Persian corpora with various writing styles from numerous subjects (e.g., scientific, novels, news) with more than 3.9M documents, 73M sentences, and 1.3B words. |
| ParsPer<br>[[website]](https://sites.google.com/site/mojganserajicom/home/parsper) [[download]](https://sites.google.com/site/mojganserajicom/home/parsper/parsper-1/model_ParsPer.tar?attredirects=0&d=1) | ParsPer is created by training the graph-based MateParser on the entire Uppsala Persian Dependency Treebank (UPDT) with a selected configuration. |
| GPT-2 Persian<br>[[website]](https://huggingface.co/bolbolzaban/gpt2-persian) [[demo]](https://huggingface.co/bolbolzaban/gpt2-persian) | Bolbolzaban/gpt2-persian is a generative language model that is trained with hyper parameters similar to standard gpt2-medium with two differences on 27GB of texts collected from different Farsi websites. More [details](https://medium.com/@khashei/a-not-so-dangerous-ai-in-the-persian-language-39172a641c84) |
| ALBERT-Persian<br>[[website]](https://huggingface.co/m3hrdadfi/albert-fa-base-v2-clf-persiannews) | The model was trained based on Google's ALBERT BASE Version 2.0 over various writing styles from numerous subjects (e.g., scientific, novels, news) with more than 3.9M documents, 73M sentences, and 1.3B words, similarly to ParsBERT. More [details](https://github.com/m3hrdadfi/albert-persian) |
| ARMAN<br>[[website]](https://huggingface.co/alireza7/ARMAN-MSR-persian-base) | ARMAN is a language model with specifically designed pre-training objectives to perform well in Persian abstractive summarization. More [details](https://github.com/alirezasalemi7/ARMAN) |
